\documentclass[5p,authoryear]{elsarticle}
\usepackage{fancyvrb}
\usepackage[utf8x]{inputenc}
\usepackage{natbib}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{url}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{color}
\usepackage{microtype}
\usepackage{booktabs}
\usepackage[hidelinks]{hyperref}
\usepackage{enumerate}


\usepackage{draftwatermark}
\SetWatermarkScale{5}
\SetWatermarkLightness{0.95}

\tolerance=800

\journal{Astronomy and Computing}

\begin{document}

% Standardised display of inline code snippets.
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\degrees}[1]{$#1^\circ$}

% Will be inserted by the build process.
@GIT_CMD@

\begin{frontmatter}

% \title{Automated imaging of radio-synthesis data for transient surveys with existing observatories \tnoteref{git}}
\title{\texttt{AMIsurvey}, \texttt{chimenea} and other tools:\\ Automated imaging for transient surveys with existing radio-observatories \tnoteref{git}}
\tnotetext[git]{Generated from git source \texttt{\githash} dated \gitdate.
Source available at \href{https://github.com/timstaley/automated-radio-imaging-paper}{github.com/timstaley/automated-radio-imaging-paper}
}

\author[oxford]{Tim D. Staley}
\ead{tim.staley@physics.ox.ac.uk}
\author[oxford]{Gemma E. Anderson}

\address[oxford]{Department of Physics, University of Oxford, Denys Wilkinson Building, Keble Road, Oxford, OX1 3RH, UK}

\begin{abstract}
In preparing the way for the Square Kilometre Array and its pathfinders, there is a pressing need to begin probing the transient sky in a fully robotic fashion using the current generation of radio telescopes. Effective exploitation of such surveys requires a largely automated data-reduction process.
This paper introduces an end-to-end automated reduction pipeline, \texttt{AMIsurvey}, used for calibrating and imaging data from the Arcminute Microkelvin Imager Large Array. 
\texttt{AMIsurvey} makes use of several component libraries which have been packaged separately for open-source release. 
The most scientifically significant of these is \texttt{chimenea}, which implements a telescope-agnostic algorithm for automated imaging of pre-calibrated multi-epoch radio-synthesis data, making use of CASA subroutines for the underlying image-synthesis operations.
At a lower level, \texttt{AMIsurvey} relies upon two libraries, \texttt{drive-ami} and \texttt{drive-casa}, built to allow use of mature radio-astronomy software packages from within Python scripts. 
These packages may be of wider techical interest, since they demonstrate use of the Python library \texttt{pexpect} to emulate terminal interaction with an external process. 
This approach allows for rapid development of a Python interface to any legacy or externally-maintained pipeline which accepts command-line input, without requiring alterations to the original code.



\end{abstract}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword
methods: data analysis \sep  techniques: interferometric
\end{keyword}

\end{frontmatter}

\section{Introduction}
\label{sec:intro}
The science of transient astronomical events is a burgeoning sub-field of astronomy. 
For the past decade, optical and gamma-ray transient surveys have led the way, with radio-band observations being restricted to follow-up of targets discovered at other wavelengths.
However, a new generation of radio facilities composed primarily of Square Kilometre Array (SKA) pathfinders such as ASKAP, MeerKAT, LOFAR, MWA, and the JVLA \citep{Norris2013}, are now at various stages of commissioning and planning, with some (LOFAR, JVLA, MWA) already producing new surveys and the rest scheduled for commissioning within the next few years.
These new observatories, and in time the SKA, will provide wider fields of view, 
greater sensitivity and potentially much faster response to targets of opportunity than was previously possible. 
Consequently, dedicated surveys in the radio-band will soon provide a stream of transient events and discoveries comparable to those discovered at other wavebands \citep{Burlon2015,Corbel2015,Macquart2015,Stappers2013}.

In preparing the way for the SKA and its pathfinders, there is a pressing
need to begin probing the transient sky in a fully-robotic fashion 
using the current generation of radio telescopes, in order to develop the 
techniques and infrastructure that will be required to make effective use of the
new observatories when they come online.  

Since 2012 we have been using the Arcminute Microkelvin Imager Large Array \citep[AMI-LA,][]{Zwart2008} to conduct a programme of automated radio-followup of transients detected at other wavelengths, now known as the AMI-LA Rapid Response Mode (ALARRM).
ALARRM has produced some of the earliest 
gamma-ray burst (GRB) follow-up in the radio \citep{Staley2013,Anderson2014},
and unprecedented early-time coverage of a radio-flare from an M-dwarf
star accompanying a hard X-ray outburst \citep{Fender2015}.

Existing facilities such as AMI-LA typically rely upon mature data-reduction pipelines created with an interactive, user-intensive model in mind, making them ill-suited for integration into fully-automated analyses of the sort needed for a quick evaluation of transient candidates. 
However, these legacy pipelines are the result of many developer-years of effort in terms of algorithm development, software engineering, and testing.
As such they represent an irreplaceable part of the data-reduction process for existing observatories. 

In the course of the ALARRM programme we have developed modular software, written in Python, to automate the data-reduction process. 
We have recently open-sourced this software as four separate packages.\footnote{%
The packages described in this paper are available from\\
\url{https://github.com/timstaley/drive-ami},\\
\url{https://github.com/timstaley/drive-casa},\\
\url{https://github.com/timstaley/chimenea}, and\\
\url{https://github.com/timstaley/amisurvey}.
}
The first two, \texttt{drive-ami} and \texttt{drive-casa} \citep{Staley2015_amisurvey, Staley2015_drivecasa}, are libraries developed to provide interfaces to mature radio-astronomy software packages, to enable their use from within Python scripts.
A third library, \texttt{chimenea} \citep{Staley2015_chimenea}, implements an heuristic algorithm for automated imaging of pre-calibrated multi-epoch radio-synthesis data, using \texttt{drive-casa} to build upon subroutines from CASA \citep[the Common Astronomy Software Applications package,][]{McMullin2007, Casa2011}.
The final package, \texttt{AMIsurvey} \citep{Staley2015_amisurvey}, ties together the previous codes to create a telescope-specific end-to-end pipeline, which ingests raw AMI-LA datafiles and produces calibrated and cleaned multi-epoch images ready for source extraction and transient identification.

This paper is structured as follows: 
Section~\ref{sec:software} introduces common methods for 
interfacing with legacy codes, discusses their relative pros and cons, 
and recalls existing examples of these 
methods in radio-astronomy software. 
% 
In Section~\ref{sec:drive-ami} we introduce AMI-LA, and describe how we have employed the \texttt{pexpect} Python library to create a scriptable interface to its associated calibration pipeline.
% 
Section~\ref{sec:drive-casa} discusses our choice to build upon the CASA routines to implement the imaging stage, and explains the rationale and design of our external
scripting interface, \texttt{drive-casa}.
Section~\ref{sec:chimenea} describes the automated imaging algorithm implemented in the \texttt{chimenea} package, and Section~\ref{sec:amisurvey} gives a brief overview of the end-to-end ALARRM data-reduction pipeline encoded by \texttt{AMIsurvey}, and presents a basic `quality control' analysis demonstrating that the pipeline behaves as expected.
% Finally, Section~\ref{sec:discuss} gives some thoughts on the importance of interoperability in future astronomical software, and we conclude in Section~\ref{sec:conclusion}.
Finally, we summarise and suggest some guidelines for writing re-usable Python codes in Section~\ref{sec:conclusion}.

\section{Methods for scripting legacy software}
\label{sec:software}

\subsection{Cross-language bindings}
Typically, the core algorithms of data-reduction pipelines are 
implemented in a lower-level language such as Fortran, C or C++, either for 
performance reasons or simply because these languages were most suitable
when the software was initially written. However, it is generally recognised
that high-level programming languages such as Python, Perl or IDL provide a more
suitable environment for rapid prototyping and development of scientific 
algorithms, due to their flexibility, succinctness, and 
convenient access to a large variety of external libraries for tasks such
as data-visualization or statistical analysis.
The standard approach to making routines written in a low-level language 
available from a higher-level language is to 
implement cross-language ``bindings'', 
specialized modules which map data-structures and function calls between the two.
Examples of this approach in radio 
astronomy are \texttt{miriad-python} \citep{Williams2012}, which provides 
Python bindings for \texttt{miriad} \citep{Sault1995,Sault2011},
and the \texttt{Obit / ParselTongue} packages 
\citep{Cotton2008,Cotton2013,Kettenis2006, Kettenis2012}, which provide 
Python bindings for AIPS \citep{Fomalont1981}. 
We refer the reader to \cite{Williams2012} for further discussion on these
packages and their merits.

While binding modules are usually the most desirable solution for developing a scriptable interface to routines written in a low-level language, there are associated costs and requirements that may make such an approach impractical. 
In astronomy, the primary issues with software development are usually those of available developer time and expertise.
Astronomers with knowledge of both high-level and low-level languages, and how to implement bindings between them, are rare. 
Even if someone with the requisite skills is available, the task of creating bindings and porting the required pipeline-logic to a high-level language may require a significant investment of developer time, both to understand the logic implemented in the legacy system, and to verify that it is faithfully reproduced in the `ported' version. 
At the end-user level, there is also the problem of familiarity: a cross-language port of a well-used legacy package will require users to re-learn the interface via its new bindings. 
Finally, there are the twin issues of access to source code, and maintenance --- 
if a tool is proprietary, or if the source is unavailable for other reasons, or non-trivial to compile, then this presents an additional hurdle to implementation and use of bindings. 
Any changes to the original package must also be reflected in the bindings modules, requiring code-maintenance and recompilation on a schedule tied to the external package.

As such, implementing binding modules from scratch may be 
overkill.
% an expensive means of achieving interoperability,  when
%  other, cruder methods might suffice. 
Some alternative approaches to using low-level or legacy codes from a higher-level language are described below.

\subsection{Sub-process calls}
If an external tool is usually invoked via an executable which takes command line arguments and then requires no further interaction, it is often trivial to invoke in a scripted fashion (otherwise known as `spawning' or `calling out' to a sub-process). 
The built in Python library \texttt{subprocess} provides a perfectly adequate interface when this is the case. 
We also direct the reader towards the \texttt{sh} 
package\footnote{
\url{http://amoffat.github.io/sh/}
}, which 
provides an alternative interface in which external commands may be represented by objects akin to Python functions, making for a neater syntax in repeated usage.

However, this approach falls down when invoking longer-running, 
multi-step processes that may require user interaction. 
While such tools often provide an option
to run a series of commands from a script, parsing the concatenated output 
from many individual commands to record details of the data-reduction, 
or worse yet detecting error conditions and locating the cause, 
can present a considerable challenge. 
In addition, larger, interactive tools often incur a few seconds start-up
time while the environment loads, and so repeatedly invoking the tool for 
multiple smaller steps can be impractically slow. 

\subsection{Terminal emulation}
\label{sec:emulation}
A third option is that of terminal emulation: spawn the external tool in a 
sub-process, but then interact with it by emulating interactive terminal input.
While this is slightly more involved than a simple sub-process call it 
allows for a one-to-one mapping of the sub-routines presented by the external 
tool into the user's scripting language of choice, without any of the 
source modification or recompilation that implementing bindings requires. 

Implementation of the basic interface class is relatively simple. 
The essential parameters are a regular expression representing the format of the command-line prompt presented by the external tool, the path to invoke the external tool, and any required environment variables.
Once the underlying interface is in place, end-user scripts that would previously be invoked from within the external tool's interactive environment can immediately be employed as part of a larger pipeline. 
Furthermore, the precise inputs passed via the interface can be recorded alongside any data-outputs, providing data-provenance, so that if an end-user later wishes to inspect the data-reduction process manually, they can load up the external tool and step through the recorded command-script, either verifying or varying the reduction steps as required.

From this starting point, it is straightforward to build up a more comprehensive interface suitable for use in complex logic flows. Since the output from each command is available in a clearly separated block of text, it is often trivial to implement some very basic parsing routines that can recover information from the command-line output of the external tool, allowing for `if / else' logic branches in the larger pipeline which make use of intermediate outputs from the external tool. 
If desirable, convenience routines can be added to ease the process of generating the commands passed, providing an interface which is better suited to the high-level scripting language (while still allowing direct access to the external tool as required).

Since this approach effectively treats an external tool's command-line interface as a programming interface, any updates to the external tool are unlikely to require changes to the scripting-interface, unless they involve changes that also break user-interaction. 
(With one caveat --- if output-parsing relies on a particular output format, this may be changed to another layout which is still human-friendly but breaks the fixed parser rules.)

In terms of performance, terminal emulation sits somewhere between that of cross-language bindings and simple sub-process invocation. 
Small delays (typically around 0.05 seconds) in the input-output cycle must be allowed for when emulating terminal interaction, presumably due to the small but measurable time it takes for the external tool to parse the commands input. In addition, any overhead incurred when starting the external tool is still present. 
However, once initialized, an instance of the external tool can be re-used for many commands, and so this start-up overhead is incurred fewer times when compared to simple sub-process calls. 

It is this terminal-emulation approach that we adopted in implementing Python interfaces to the \texttt{AMI-REDUCE} and CASA packages, as detailed below.


\section{Automated calibration of AMI-LA data with \texttt{drive-ami}}
\label{sec:drive-ami}
The Arcminute Microkelvin Imager Large Array (AMI-LA) is a synthesis telescope composed
of eight equatorially mounted 12.8 m dishes sited at the Mullard
Radio Astronomy Observatory (MRAO) at Lord’s Bridge, Cambridge. The
telescope observes in the band 12.5–18.2 GHz with eight 0.72 GHz
bandwidth channels. 
% Since early 2012 a rapid-response programme, the `AMI Large Array Rapid Response Mode' (ALARRM) has been in place to perform rapid radio follow-up for Swift detections of GRBs \citep{Staley2013,Anderson2014,Fender2015}. 

AMI-LA observations are recorded in a custom data format to be processed by a specialized pipeline, \texttt{AMI-REDUCE}. 
This applies path delay corrections, automatic flags for interference, pointing errors, shadowing and hardware faults, applies phase and amplitude calibrations, Fourier transforms the data into the frequency domain, and writes out the resulting data in \textit{uv}-FITS format \citep{Davies2009}. 
Typically data reduction is either carried out interactively, allowing the user to view plots of the data in order to guide the reduction process, or by calling one of a selection of standard reduction scripts (which are simple listings of \texttt{AMI-REDUCE} commands, without any flow control such as `if' statements or `for' loops.). 

Application of one of the standard \texttt{AMI-REDUCE} listings was adopted as a calibration step for observations made as part of the ALARRM programme. 
Early versions of the ALARRM pipeline ran \texttt{AMI-REDUCE} via a simple subprocess call, but this proved problematic. 
First, when inspecting the logs, a successful reduction with \texttt{AMI-REDUCE} was indistinguishable from an aborted reduction due to a problematic dataset. 
Second, important metadata such as the rain modulation (which gives some idea of the data-quality) and observation time is not stored in the header of the \textit{uv}-FITS output files, and so checking these values still required manually loading the dataset in question into the \texttt{AMI-REDUCE} pipeline. 
A more powerful programmatic interface to \texttt{AMI-REDUCE} was required.

The \texttt{AMI-REDUCE} pipeline is implemented in \mbox{FORTRAN} 77, and comprises around 58,000 lines of 
source-code\footnote{As analyzed using the \texttt{SLOCCount} utility: \\ \url{http://www.dwheeler.com/sloccount/}\\
(Including component libraries.)}. 
In the rare case that \texttt{AMI-REDUCE} needs to be run elsewhere than the MRAO it is typically distributed as a pre-built binary, with a copy of the source files included. 
Despite the author's efforts to install appropriate dependencies and alter makefiles, building the package from source on a modern Ubuntu Linux installation proved to be problematic (as opposed to simply executing the pre-built binary, which was simple once the appropriate compatibility packages had been downloaded and installed). 
Given the size and complexity of the codebase, the difficulties building from source, and the authors' lack of expertise in Fortran, we judged that implementing Python-bindings to the \texttt{AMI-REDUCE} routines directly would require a significant and costly investment of developer time, with little benefit given that \texttt{AMI-REDUCE} is not a widely-used software package. Instead, we implemented a terminal-emulation interface that provides a set of Python routines for controlling \texttt{AMI-REDUCE}, under the moniker \texttt{drive-ami}. 
To do so we make use of 
\texttt{pexpect}\footnote{%
\url{http://pexpect.readthedocs.org}}, 
a pure-Python library for emulating terminal-interaction.  
\texttt{pexpect} is small (a 132kB download) and only requires a working Python installation (v2.6 or greater) as a prerequisite. 

In addition to allowing step-by-step calling of the \texttt{AMI-REDUCE} functionality from a Python script, \texttt{drive-ami} provides a number of enhancements to the data-reduction process.
First, as discussed in Section~\ref{sec:emulation}, data-provenance and reproducibility is improved. 
All the \texttt{AMI-REDUCE} commands are recorded in a log-file along with the output \textit{uv}-FITS data. 
In a separate file, all textual output displayed during the reduction process is logged alongside the input commands, so that the end-user may read back over the reduction dialogue as if scrolling back through a terminal history arising from using \texttt{AMI-REDUCE} interactively. 
A variety of metadata such as observation times, calibrator source, rain modulation and percent of data flagged due to radio-frequency interference is also parsed from the output and stored in JavaScript Object Notation (JSON) format, which has the benefit of being both human-readable and trivially parsed in most high-level programming languages. 

As an added convenience, \texttt{drive-ami} can also be used to help configure reduction of multiple-epoch data for a source observed with AMI-LA. 
While observations are usually easy to sort by their filename-prefix (which refers to the target name), occasionally a target is renamed after the initial epoch of observation (e.g. to reflect an assigned GRB ID), or a prefix may be incremented if multiple observations are performed in a single day. 
\texttt{drive-ami} can utilise \texttt{AMI-REDUCE} to extract the pointing information from the raw data-files and then group observations according to their pointing, given a tolerance limit on the maximum angular separation between pointing-centres. 
We make use of the Astropy co-ordinate routines for calculating the angular separation \citep{Astropy2013}.
The JSON-encoded metadata then contains a `group' tag, which can be used to identify all observations of a given target for processing as a group (Section~\ref{sec:chimenea}).


% Bit of a hack:
% Drop this in way earlier than necessary to achieve desired diagram /text ordering 
\begin{figure*}[p!]
\begin{center}  
  \includegraphics[width=.96\textwidth]{../figures/chimenea-flow-bold-labels}
  \caption[Overview of \texttt{chimenea} logic]{%
An overview of the algorithm implemented in the \texttt{chimenea} pipeline.
Dotted lines denote re-use of data-products from an earlier stage. 
Groupings labelled in Roman numerals refer to descriptions in text.
The terms `open' and `masked' Clean refer to applying the Clean
algorithm \citep{Schwab1984} either to a box-region covering the central quarter of a map (i.e. an open or unconstrained clean), or to a masked map, i.e. one where the Clean algorithm is constrained to placing model components in the vicinity of known sources.
The `re-Clean' subroutine is depicted in Figure~\ref{fig:reclean}.
\label{fig:chimenea}
} 
\end{center} 
\end{figure*}


\section{External scripting of CASA subroutines with \texttt{drive-casa}}
\label{sec:drive-casa}
Traditionally, the \textit{uv}-FITS data products output from the \texttt{AMI-REDUCE} reduction stage are reduced using the \texttt{AIPS} package \citep{Fomalont1981}, but for the ALARRM programme we chose to make use of CASA \citep{McMullin2007}. 
We made this switch for a few reasons. 
We judge \texttt{CASA} as the best supported general-purpose package for radio-astronomy data-reduction, 
with a simple installation process, 
frequent updates\footnote{\url{http://casa.nrao.edu/previous_casa.shtml}}, 
a comprehensive accompanying 
manual\footnote{\url{https://casa.nrao.edu/ref_cookbook.shtml}},
and a helpdesk service. 
\texttt{CASA} is also widely applicable, recognising data from many of the current generation of telescopes; support is also available for newly commisioned facilities such as 
LOFAR \citep{vanHaarlem2013}\footnote{%
See also the LOFAR imaging cookbook:\\ 
\url{http://www.astron.nl/radio-observatory/lofar/lofar-imaging-cookbook}
}
and ALMA. 
Lastly, the Python interpreter underlying the default CASA interface, \texttt{casapy}, provides a convenient scripting interface for many basic tasks, albeit with some restrictions, as described below.

It is trivial to run simple Python scripts within the \texttt{casapy} environment, or even to launch \texttt{casapy} into a Python script directly from the command line. 
However, problems arise if you wish to make use of functionality from both \texttt{casapy} and non-standard Python packages (i.e., anything except the standard library) from within the same script.
This is because \texttt{casapy} bundles its own Python interpreter, customized to display a logging window and provide access to plotting tools. 
As a result, Python packages installed in the normal manner are unavailable from within the \texttt{casapy} environment. 
This is unfortunate, as it precludes easy usage of CASA routines from within a larger pipeline, even though much of the underlying interface is already written in Python.

With regards to data-provenance and reproducibility, \texttt{casapy} provides extensive logfiles which provide a record of the commands and resulting information from a data-reduction run. 
However, a minor flaw is that once instantiated, \texttt{casapy} provides no method (or at least, no documented method) for redirecting log output, which can make it harder to isolate the log section relevant to a given observation. 
Logging of input and output is also collapsed into a single stream, which makes it tricky to extract the commands required to reproduce a reduction process.

Programmatic error-handling is also lacking from the \texttt{casapy} environment. 
The standard \texttt{casapy} routines do not provide a return value, or throw exceptions on encountering error conditions. 
Instead, all processing information and errors are output as log messages, which makes it hard to respond in an automated fashion.

Our package for automating interaction with \texttt{casapy}, `\texttt{drive-casa}', addresses these issues.
Similarly to \texttt{drive-ami}, we use \texttt{pexpect} to emulate terminal interaction with the \texttt{casapy} process. 
This side-steps the issues of directly accessing the Python functions, albeit with a cost that we need to implement custom parsing routines to extract any required information from the \texttt{casapy} log-output. 

The \texttt{drive-casa} interface can be used to run plain-text \texttt{casapy} scripts directly; alternatively the package includes a set of convenience routines which try to adhere to a consistent style and make it easy to chain together successive CASA reduction commands to generate a \texttt{casapy} script programmatically.
For example, the following code fragment generates a script to invoke the CASA routine \mbox{\textit{importUVFITS}}, then perform a zero-iteration Clean operation on the resulting CASA \textit{MeasurementSet}
to produce a dirty map:


\begin{Verbatim}[samepage=true]
script = []
ms = drivecasa.commands.import_uvfits(
                script, 
                uvfits_path)
dirty_maps = drivecasa.commands.clean(
                script, 
                ms, 
                niter=0, 
                threshold_in_jy=1,
                other_clean_args=clean_args)
                
\end{Verbatim}

We note that the recently released package \texttt{casa-python}\footnote{%
\url{https://github.com/radio-astro-tools/casa-python}
}
should make it easier to install external Python packages into the \texttt{casapy} environment, which goes some way to addressing the issue of package interoperability whilst retaining access to the standard \texttt{casapy} logging interface and plotting tools. 
This may provide a preferable solution for those who wish to make small alterations to their \texttt{casapy} workflow to include external functionality, but still effectively treats \texttt{casapy} as the top-level pipeline framework, rather than abstracting it to a more standard Python package, as \texttt{drive-casa} does.


Since \texttt{casapy} is largely Python code internally, and is still undergoing active development, we hope that eventually a native Python interface to \texttt{casapy} will be made available, thus negating the need for the terminal-emulation layer of \texttt{drive-casa}. 
In the meantime, \texttt{drive-casa} provides a reliable method for incorporating CASA functionality into a fully automated Python reduction pipeline, as demonstrated in Section~\ref{sec:chimenea}.



\section{Automated imaging of multi-epoch radio-synthesis data with \texttt{chimenea}}
\label{sec:chimenea}
Prompt analysis of results from the ALARRM programme requires frequent re-reduction of target-datasets, as additional observations become available.
In order to minimise the human effort associated with the imaging stage of the data-reduction, we created a tool for automated imaging of pre-calibrated multi-epoch radio-synthesis data, `\texttt{chimenea}', which is a small pipeline built atop the CASA subroutines (using \texttt{drive-casa} to provide an interface layer). 

The \texttt{chimenea} pipeline is designed to automatically choose a suitable set of parameters for applying the Clean algorithm \citep{Hogbom1974,Schwab1984}, addressing two key issues: how `deep' should we Clean (i.e., at what level of peak pixel value in the residuals map do we terminate the Clean process), and how the placing of model components should be constrained via the Clean mask. 
When performing a blind search for transients, unconstrained or `open' Clean reductions are preferable, since we do not know where a transient source may appear, and an `un-Cleaned' source will be harder to detect since the flux remains spread over the core and sidelobes of the dirty beam. 
As such, when imaging a field of view which contains no detectable steady sources (to search for transient sources), a simple approach to automating this process is to estimate the root-mean-square noise-level (RMS) from the dirty map, and then apply an open-Clean process, with the Clean termination threshold set to a multiple of the estimated RMS (typically around three). 
However, this simple approach produces poor results when a bright steady source is located in the field of view, particularly if the single-epoch data being imaged is from a short integration with poor \textit{uv}-plane coverage.
In this case, the RMS as estimated from the dirty map will often be positively biased, due to additional pixel-value variance from the side-lobes of the bright source. 
Additionally, an open Clean may erroneously place model components in the side-lobes of the bright source, leading to artifical source-artefacts and reduced flux assigned to the model component at the true location of the bright source.
Therefore, when imaging a field containing a known bright source the best strategy is to perform a constrained (`masked') Clean --- but this potentially leaves transient sources `un-Cleaned', as noted previously.

To resolve this problem, \texttt{chimenea} takes an iterative approach that makes use of all available epochs of data for a given survey field.
Initially, the data from all epochs are concatenated and an open-Clean process is applied, which is fairly robust to artefacts given enough epochs of data, due to improved \textit{uv}-plane coverage. 
To ensure the RMS estimation (and hence Clean-termination threshold) is not biased by side-lobes in the dirty map, we re-estimate the RMS after applying an open-Clean and then reapply the Clean process with a lower threshold if a significant drop in RMS is detected.
Each single-epoch observation is then processed individually. 
We initially apply a masked Clean, to constrain the placement of Clean-model components to the location of any known bright sources in the field (as detected in the deep image) --- this avoids the issues of side-lobe artefacts due to poor \textit{uv}-coverage in shorter observations. 
Finally, we apply an open Clean to each epoch in case a transient source is detected.

Figure~\ref{fig:chimenea} provides an overview of the implemented data-flow. 
A step-by-step description of the process, relating to the different stages denoted in Figure~\ref{fig:chimenea}, is as follows:

\begin{enumerate}[I]
 \item Concatenating data from all epochs gives better \textit{uv}-coverage and produces a `deep' map with lower noise, which is ideal for reliably locating steady sources in the field of view. To generate this deep map we apply the Cotton-Schwab variant of the Clean algorithm \citep{Schwab1984} to the concatenated data, allowing unconstrained placing of model components within a box-region corresponding to the central quarter of the synthesized map (referred to as an `open Clean' in Figure~\ref{fig:chimenea}). 
Sources outside this central region are attenuated by a primary beam response factor of less that 1 in 20,000 relative to the pointing centre, and so a wider open-Clean region is unnecessary unless this margin happens to contain a source with flux level on the order of one Jansky. 

 \item Applying the Clean algorithm requires that we give a pixel-value termination threshold, which is typically chosen as a multiple of the background root-mean-square (RMS) noise level. 
 However, in order to estimate the RMS, we require an image to sample. 
 To overcome this `chicken-and-egg' problem we take an iterative approach (`re-Clean'), as depicted in Figure~\ref{fig:reclean}. 
 Beginning with the dirty map, we take the pixel values from the residuals map (i.e. the image with any Clean-model components subtracted), apply sigma-clipping to estimate the background RMS,  apply Clean with the resulting calculated threshold, and then iterate by examining the new residuals map.
 Our chosen convergence criteria for ending the process is to stop when the proportional decrease in RMS estimates between iterations is less than a user-chosen value (typically 5\%) or when a set number of iterations have completed (typically three), whichever comes first. 
 For the sigma-clipping and RMS estimation step we employ the algorithm described in Section 4.3.1 of \cite{Swinbank2015}, which uses information on the synthesized beam shape to correct for bias in the RMS estimate caused by inter-pixel correlation.
 
 \item Applying an initially unconstrained iterative re-Clean process to each single-epoch observation can give poor results, especially if bright steady sources are present in the field of view, as poor \textit{uv}-coverage can lead to misidentification of side-lobes as separate sources,  and over-estimation of background RMS levels. 
 However, since the deep image is generated from multiple concatenated observations it typically has much better \textit{uv}-coverage and sensitivity. Therefore, we use the deep image to identify steady sources and generate a Clean mask. 
 This mask is used when imaging single-epoch data to constrain the Clean process to place model components only in the vicinity of known sources. 
 To identify sources in the deep image we make use of a source-extraction tool tailored to radio-data and currently distributed as part of the LOFAR-TKP transients pipeline \citep{Trap2014,Swinbank2015}.
 The user can also specify co-ordinates of known sources to create additional mask apertures, in order to ensure proper cleaning of any known faint sources or transient candidates. 
 
 \item  Applying the constrained (via use of the Clean aperture-mask) re-Clean algorithm to the single-epoch observations is generally quite robust and provides a good estimate of the background RMS at each epoch, in addition to accurately cleaning sources previously detected in the deep image.
 
 \item We can now apply an unconstrained Clean operation to each single-epoch observation, down to the flux-threshold determined by the previous masked re-Clean process, in case there are single-epoch transient sources present which do not correspond to a mask aperture. 
 We initialize this Clean operation with the source-component model derived from the masked Clean process, which should help to ensure that any bright sources in the field are cleaned correctly, without spurious assignment of source-components to bright side-lobe artefacts.
 
 \item Finally, the pipeline applies primary-beam corrections, and outputs images in both CASA \textit{MeasurementSet} and FITS formats.
\end{enumerate}



\begin{figure}
\begin{center}  
  \includegraphics[width=0.45\textwidth]{../figures/reclean-flow}
  \caption[Iterative `re-Clean' subroutine logic.]{%
A schematic of the iterative `re-Clean' subroutine, applied as part of the \texttt{chimenea} algorithm depicted in Figure~\ref{fig:chimenea}.
The subroutine repeatedly applies the Clean algorithm until acceptance criteria are met. 
The convergence tolerance (shown as 5\% in figure), and the Clean threshold as a multiple of the estimated root-mean-square background noise level (RMS), are both user-configurable. 
An additional termination criterion is applied --- maximum number of iterations around the Clean / re-estimate RMS loop (typically three) --- but is omitted from the chart for brevity.

\label{fig:reclean}
} 
\end{center} 
\end{figure}

We note that the data-flow currently implemented in \texttt{chimenea} represents an heuristic approach to automated radio-synthesis imaging, and may change in the future as we gain further experience.
The current pipeline is already a considerable improvement over our initial automated solution (which was to simply perform an independent open Clean on each single-epoch observation) but particularly challenging or scientifically significant datasets will still warrant manual care and attention; \texttt{chimenea} provides a reduction which can sometimes be improved further but is good enough for scientific analysis in most cases.
Additionally, \texttt{chimenea} represents significant value as a working software-implementation, complete with extensive logging, a clearly structured logic-flow, and a number of simple data-structures which aim to make tractable the problem of managing numerous intermediate datasets and configuration parameters. 
We hope that others will be able to use and build upon this initial implementation.

To encourage re-use, the \texttt{chimenea} package has been kept as simple as possible, consisting solely of some basic data structures and the subroutines that make up the algorithm described above, with the complexities of calling out to \texttt{casapy} delegated to the \texttt{drive-casa} layer. 
However, this minimalist approach means that there is no command-line interface or default parameter set. 
These needs are met by the \texttt{AMIsurvey} package, described below.


% \section{Discussion: considerations for future astronomical software development}
% \label{sec:discuss}
% 
% 
\section{\texttt{AMIsurvey}, an end-to-end pipeline for automated reduction of AMI-LA observations}
\label{sec:amisurvey} 
\begin{figure}
\begin{center}  
  \includegraphics[width=0.4\textwidth]{../figures/amisurvey-deps}
  \caption[Dependency chart for the \texttt{AMIsurvey} package.]{%
A dependency chart for the \texttt{AMIsurvey} package; \texttt{AMIsurvey} depends on \texttt{drive-ami}  which in turn depends on the external package \texttt{AMI-REDUCE}, etc.
The \texttt{tkp} block refers to the LOFAR-TKP software package \citep{Trap2014} described in \cite{Swinbank2015}.
Note that only the salient dependencies are listed; in reality we additionally make use of a 
number of external Python packages.
\label{fig:amisurvey-deps}
} 
\end{center} 
\end{figure}

The top-level tool used for data-reduction in the ALARRM programme is \texttt{AMIsurvey}. 
This package glues together the calibration and imaging processes implemented via \texttt{drive-ami} and \texttt{chimenea}, depending in turn upon the CASA and \texttt{AMI-REDUCE} packages (cf Figure~\ref{fig:amisurvey-deps}). 
We make use of the the Python standard library package \texttt{argparse} to provide a user-friendly command line interface.
Initially, the \texttt{AMIsurvey} tool loads lists of calibrated \textit{uv}-FITS files and associated metadata from the JSON files output by \texttt{drive-ami}. 
The data-locations and metadata are then trivially converted into the data-structure used by the \texttt{chimenea} pipeline, before being sorted into groups. 
These calibrated observations may be filtered according to quality-control criteria (e.g. rain amplitude modulation) to identify and remove any problematic observations.
The grouped-and-filtered observations are then passed, along with configuration parameters (e.g. Clean image-synthesis settings) appropriate to the ALARRM data, to the main \texttt{chimenea} pipeline function for processing. 
Finally, a full listing of the data-products is output in JSON format, for easy ingestion by further analysis tools. 

To thoroughly test the \texttt{AMIsurvey} pipeline, we reduced a significant portion of the ALARRM data obtained to date: 1035 observations spread over 165 target fields, representing approximately 100 days of telescope time with AMI-LA (approx. 100\,GB of raw data). 
The calibration and imaging stages took around eight and six hours respectively on a single CPU-core. 
The metadata associated with these reductions were then analysed to confirm that the pipeline generally behaves as expected, identify any problematic datasets, and place some quantitative values on the benefits of applying the full \texttt{chimenea} algorithm, as opposed to simply applying a single Clean to each observation.\footnote{%
 The \texttt{IPython} notebook used to perform this analysis is included with the \texttt{AMIsurvey} package, and a static version relating to the dataset described herein may be viewed at  \url{http://nbviewer.ipython.org/gist/timstaley/5ef6dfc2a370e0288bb0}.
}


The first step of the analysis was to exclude any fields of view for which no sources were detected in the concatenated deep-image at the 5.5 sigma level --- these fields have only a single open Clean operation performed at each epoch, based on the RMS estimated in the dirty map, and so do not provide relevant datapoints for this comparison.
This left 862 observations spread over 121 target fields where a source was detected in the deep image.
The data were reduced with (somewhat arbitrarily chosen) re-Clean convergence criteria of a RMS-decrease less than 5\% after applying a Clean operation, or a maximum of three re-Clean iterations (cf Figure~\ref{fig:reclean}). 
For the ALARRM dataset, these values appear quite suitable --- 503 observations converged after one iteration (i.e. less than 5\% difference in estimated RMS between dirty map and image produced from one masked Clean operation), 296 converged after two iterations, leaving 63 observations for which the re-Clean algorithm iterated the maximum of three times. 
Of these 63 remaining observations, 62 had an RMS decrease in the final re-Clean iteration of less than 5\%. Therefore we can consider only one observation as requiring a `hard-stop' on the re-Clean iterations. 
(This non-converging observation turns out to be from a field with extended emission, which is generally problematic for the current \texttt{AMIsurvey} pipeline.)

As an additional check on the pipeline's behaviour, we produced Figures~\ref{fig:rms}~and~\ref{fig:rms_long_duration}, which compare the fractional decrease in RMS-estimate between the dirty map and the final image, and the flux density of the brightest source detected in the deep image. 
Generally, we expect fields with brighter sources to exhibit higher levels of side-lobe artefacts in the dirty map, which positively bias the initial dirty-map RMS-estimate (particularly for short observations with poor \textit{uv}-coverage), and so we expect some level of correlation between these two quantities. 
The resulting plots show a smooth progression in the maximum RMS-decrease with increasing source flux, excepting some well-understood outliers:
\begin{itemize}
   \item The observations of XTEJ908+094 are affected by a very bright out-of-field source, which presents extended side-lobe artefacts as additional background variation in the field-of-view.
   \item The PTF09AXC field contains extended emission and/or blended sources (which are not well suited to reduction by the automated pipeline).
   \item The observations of field SWIFT\_554620 contain a single epoch with a particularly bright source (GRB140327A in afterglow), and so this observation falls lower on the  plot than the rest of the grouping.
\end{itemize}
In total there are 84 observations of fields with sources brighter than 10\,mJy, which are not shown in Figures~\ref{fig:rms}~and~\ref{fig:rms_long_duration}.
When plotted, these datapoints continue the general trend with increased scatter, but we do not consider them as suitable for automated reduction in the standard \texttt{AMIsurvey} pipeline as modified calibration settings need to be applied when a source of around 10\,mJy or brighter is in the field of view.

As can be seen in Figure~\ref{fig:rms}, when a bright source is present the RMS-estimates may drop by a factor of two or more after the masked re-Clean process is applied. 
Since this RMS-estimate is then used when applying the open-Clean process to look for single-epoch transients, it suggests a potentially significant improvement in sensitivity to transient sources when a bright steady source is present in the field of view. However, some observations of fields with a bright steady source do not show such improvement. This is to be expected if short observations, or those which undergo a high degree of RFI flagging, have sufficiently poor data quality that bright source sidelobes are not a significant contributing factor to background variation. To test this hypothesis, we made a simple cut on the data by replotting only observations of duration greater than 3.5 hours, as shown in Figure~\ref{fig:rms_long_duration} (leaving 289 observations spread over 65 different targets). This plot shows a much clearer relation between bright-source flux-density and RMS-estimate reduction.


Finally, we note that experience suggests the trade-off between sensitivity to real transients and artefacts in radio-astronomy is difficult to quantify without extensive testing and simulation, which we leave to future investigation.

\begin{figure*}[p]
\begin{center}  
  \includegraphics[width=0.8\textwidth]{../figures/rms_to_2mjy}
  \includegraphics[width=0.8\textwidth]{../figures/rms_to_10mjy}
%   \captionsetup{singlelinecheck=off}
  \caption[Fractional RMS decrease vs. brightest source flux]{%
  \label{fig:rms}
  Scatter-plots comparing the fractional decrease in RMS-estimate (the ratio of [final image RMS / dirty map RMS] for each single-epoch observation), and the flux density of the brightest source detected in the deep image of that observation's field of view. 
  Each target has a single value for `brightest source-flux in deep image,' but RMS-decrease varies between epochs, resulting in all RMS-decrease values for a single target being plotted as a column of points.
  As expected, fields with brighter sources in view often have more drastically reduced RMS levels than fields with only faint sources.
  A few outliers (overplotted in red and green) buck this trend, for well understood reasons --- details in text.
  Upper and lower sub-plots depict the same data but with different limits on plotting region applied, to give more clarity in the densely populated upper-left region.
} 
\end{center} 
\end{figure*}

\begin{figure*}[p]
\begin{center}  
  \includegraphics[width=0.8\textwidth]{../figures/rms_to_2mjy_4hrs}
  \includegraphics[width=0.8\textwidth]{../figures/rms_to_10mjy_4hrs}
%   \captionsetup{singlelinecheck=off}
  \caption[Fractional RMS decrease vs. brightest source flux for long-duration observations]{%
  \label{fig:rms_long_duration}
  As Figure~\ref{fig:rms}, but this time only observations of duration 3.5 hours or greater (typically 4 hours) are plotted. 
  With the short-duration observations removed, the plots show a clearer relation between the flux-density level of the brightest source in the field, and the RMS-estimation improvement gained by applying an iterative Clean process.
} 
\end{center} 
\end{figure*}


\section{Summary and discussion: writing re-usable Python codes}
\label{sec:conclusion}
In sections~\ref{sec:drive-ami} and \ref{sec:drive-casa} we explored the potential for using terminal-interaction emulation as a means of rapidly developing a Python interface to externally developed pipeline tools which provide a command-line interface, as an alternative to cross-language bindings. 
We found this approach relatively easy to implement and well suited to our needs, especially given the added benefits of easy manual reproducibility and data-provenance. 
In Section~\ref{sec:chimenea} we introduced \texttt{chimenea}, a pipeline for automated imaging of radio-synthesis data. 
Although heuristic in approach, \texttt{chimenea} already provides a useful component in automation data-reduction pipelines, and serves as a test-bed and benchmark for more sophisticated reduction algorithms. 
We note that extensive further development in the area of automated imaging will be essential for performing real-time image-plane transient surveys, one of the key science drivers for the SKA \citep{Burlon2015,Corbel2015}.
In Section~\ref{sec:amisurvey} we described how we have integrated these packages into a single tool that enables fully-automated data reduction for the ALARRM survey programme, and presented some basic checks of pipeline integrity.

This work also highlights a pressing issue for the field of astronomy as a whole (and wider academia): that of software interoperability and re-use. 
There will always be a trade-off to be made in choosing to incorporate legacy packages into new efforts, or starting afresh with a re-write in the language \textit{du jour} --- nothing has changed there. 
However, when writing new software, we would encourage authors to consider how their code might be adapted for re-use, either in part or in whole. 
In the wider context of software development, situations are so varied that it is hard to give any firm advice on this topic (or at least,  beyond the scope of this work). 
However, if we restrict our scope to that of writing scientific codes in Python, we can be a little more concrete; giving a few examples from this work that hopefully serve to illustrate the wider argument:
\begin{itemize}
 \item Prefer interoperable, modular libraries over monolithic pipeline packages 
  (this is a widely discussed viewpoint more generally known as the \textit{Unix philosophy}). 
  The packages described in this paper began life as one big collection of prototype code that called out to each external tool in turn (via a sub-process call).
  As we added features and the codebase grew, it became clear that it was worth separating into separate packages.
  
 \item Use the Python packaging infrastructure. 
  A major hurdle to breaking up a code into sub-packages is that it induces additional dependency management.
  Embracing the Python packaging infrastructure to implement a versioned install process helps keep this under control, and also makes it much easier to distribute the code to users not directly involved in the development 
  process.\footnote{A guide to creating installable Python packages may be found at \href{http://the-hitchhikers-guide-to-packaging.readthedocs.org/}{\url{http://the-hitchhikers-guide-to-packaging.readthedocs.org/}}}

 \item Ideally, distinguish technical implementation details from higher-level applications of code (or `low-level' vs `high-level' programming interfaces). 
    Core libraries should ideally be simple (each function does one thing well) and flexible (functions can be used independently, perhaps requiring a number of parameters, etc).
  However, when writing a larger software package it is essential to hide common-usage patterns of a low-level interface behind larger subroutines - this is the only way to write complex code which is readable and maintainable. 
   Think carefully about where it might be sensible to divide these code hierarchies and separate them into high-level and low-level packages. For example, in this work \texttt{drive-casa} provides a direct interface to the \texttt{casapy} package (low-level), while \texttt{chimenea} encodes the scientifically relevant logic and implements a particular view of how best to structure the data (high-level).

 \item Avoid use of the \textit{print} function. 
    The Python \texttt{logging} package provides a good degree of control and flexibility in how debugging and information messages are displayed, stored to file, distributed over the network, etc, whereas the \textit{print} function is a basic on/off output and should only ever be used as a temporary measure --- otherwise you risk overwhelming calling routines with a flood of irrelevant debugging output.
    There are also many extensions to the \texttt{logging} functionality, for example \texttt{AMIsurvey} makes use of the 
  \texttt{colorlog} package\footnote{\href{https://github.com/borntyping/python-colorlog}{\url{https://github.com/borntyping/python-colorlog}}} 
to colourise the terminal output by logging category.

 \item Separate configuration from code.
   Ideally, any scientifically significant parameters should be clearly identifiable and easily changed from one location. 
   In the current \texttt{AMIsurvey} pipeline, telescope and data specific parameters are encoded in a single, short module dedicated to the AMI-LA telescope. 
   This could easily be extended to load a different module depending on metadata entries, or load parameters from a user-edited text file as required.
%   \item Whenever possible, implement top-level logic in a library module, then call it from a front-end script.
\end{itemize}

Finally, we highly recommended that even if a scientific code follows none of this advice, it should still be made freely available. 
Stylistic changes to suit a given context can always be made after release, and the benefits of source code sharing are significant, perhaps most crucially for encouraging reproducibility \citep[see e.g.][for further discussion]{Shamir2013}.


% In short, the differences between the 
% approaches may be summarized as follows:
% 
% \noindent
% \textbf{Bindings}\\
% Pros:
% \begin{itemize}
%  \item Low performance overhead.
%  \item Enables sharing of in-memory data between low-and-high level 
%     languages.
%  \item Allows reliable detection of error conditions.
% \end{itemize}
% Cons:
% \begin{itemize}
%  \item Requires access to, and recompilation of, the original source code.
%  \item Potentially large development cost / specialized skills required.
%  \item Requires end-users to learn a new interface.
% \end{itemize}
% 
% \noindent
% \textbf{Terminal emulation:}\\
% Pros:
% \begin{itemize}
%  \item Low development cost.
%  \item No recompilation required.
%  \item Data-reduction steps are trivial to reproduce manually, allowing for
%   interactive investigation of potential improvements.
%  \item Users can pass commands directly or make use of convenience routines 
%     written in Python.
% \end{itemize}
% Cons:
% \begin{itemize}
%  \item Modest performance overhead.
%  \item Error conditions must be actively deduced from error messages or 
%       unexpected output.
%  \item Data must be passed between low-and-high level routines by writing
%   to file.
% \end{itemize}






\section*{Acknowledgements}
We thank the staff of the Mullard Radio Astronomy Observatory for their invaluable assistance in the operation of AMI.

This work made use of a number of publicly available software packages and data products: NASA's Astrophysics Data System; Astropy, a community-developed core Python package for Astronomy \citep{Astropy2013}; the IPython package \citep{PER-GRA:2007}; matplotlib, a Python library for publication quality graphics \citep{Hunter2007}; and SciPy \citep{jones_scipy_2001}. 

This project was wholly funded by European Research Council Advanced Grant 267697 
`4 $\pi$ sky: Extreme Astrophysics with Revolutionary Radio Telescopes.’


\section*{References}
% \bibliographystyle{elsarticle-harv}
\bibliographystyle{elsarticle-harv-max8}
\bibliography{automated_radio_reduction}

\end{document}
